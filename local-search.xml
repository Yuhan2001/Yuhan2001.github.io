<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文笔记：Order Matters</title>
    <link href="/2024/05/04/Order%20Matters/"/>
    <url>/2024/05/04/Order%20Matters/</url>
    
    <content type="html"><![CDATA[<blockquote><p>注：部分内容参考自GPT生成的内容</p></blockquote><h2 id="论文笔记：Order-Matters（AAAI-20）">论文笔记：Order Matters（AAAI 20）</h2><p>用于二进制代码相似性检测的语义感知神经网络</p><p>论文:《<a href="https://keenlab.tencent.com/en/whitepapers/Ordermatters.pdf">Order Matters: Semantic-Aware Neural Networks for Binary Code Similarity Detection</a>》（<strong>AAAI 2020</strong>)</p><blockquote><p>笔记参考：<a href="https://keenlab.tencent.com/zh/2019/12/10/Tencent-Keen-Security-Lab-Order-Matters/">AAAI-20论文解读：基于图神经网络的二进制代码分析 | 腾讯科恩实验室官方博客 (tencent.com)</a></p></blockquote><h3 id="动机">动机</h3><p>传统方法通常使用图匹配算法，但这些方法慢且不准确。尽管基于神经网络的方法取得了进展（如Gemini），但它们每个基本块都是以人工选择特征的低维嵌入来表示的，通常不能充分捕获二进制代码的语义信息。其次，节点的顺序在表示二进制函数时起着重要作用，而以往的方法并没有设计提取节点顺序的方法。</p><blockquote><p>另外，在Related Work中提到，(Zuo et al 2018) 使用的NLP模型也有缺点。他们通过修改编译器，在每个生成的汇编块中添加一个基本块特殊注释器，该注释器为每个生成的块注释一个唯一ID。这样，可以将来自同一源代码片段编译的两个基本块视为等效。获取相似块对是一个有监督的过程，不同操作系统或硬件架构需要训练不同的模型</p></blockquote><h3 id="方法">方法</h3><p><strong>提出的模型</strong>：</p><p>模型的输入是二进制代码函数的控制流图（CFGs），其中每个块是带有中间表示的token序列。在语义感知上，模型使用BERT预训练接受CFG作为输入，并预训练token嵌入和块嵌入。在结构感知上，使用带GRU更新函数的MPNN来计算图的语义和结构嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">g_{ss}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。在顺序感知上，采用CFG的邻接矩阵作为输入，并使用CNN来计算图的顺序嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">g_{o}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。最后将它们连接起来，并使用一个MLP层来计算图嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mrow><mi>f</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>=</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><msub><mi>g</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub><mo separator="true">,</mo><msub><mi>g</mi><mi>o</mi></msub><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g_{final} =MLP([g_{ss}, g_{o}])</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mord mathnormal mtight">ina</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">([</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">])</span></span></span></span></p><p>![image-20240121203627839](Order Matters/image-20240121203627839-17058476037342.png)</p><h4 id="语义感知模块-Semantic-aware-Modeling">语义感知模块 (Semantic-aware Modeling)</h4><p>使用BERT进行预训练，包括四个任务：掩码语言模型任务（MLM）、邻接节点预测任务（ANP）、块内图任务（BIG）和图分类任务（GC）。这些任务帮助模型提取CFG的token级别、块级别和图级别的语义信息。</p><p>![image-20240121204851147](Order Matters/image-20240121204851147-17058476023021.png)</p><ul><li><strong>掩码语言模型任务（MLM）</strong>：通过在输入层掩盖token并在输出层预测它们来提取<strong>块内的语义信息</strong>。这是一个自监督任务，模型在训练过程中某些token会被隐藏，模型必须基于其他token提供的上下文来预测缺失的token。</li><li><strong>邻接节点预测任务（ANP）</strong>：因为块的信息不仅与块本身的内容相关，还与其邻近的块相关，ANP任务旨在让模型学习这种<strong>邻接信息</strong>。它涉及提取图中所有相邻块对，并在同一图中随机抽样多个块对，以预测它们是否相邻。</li><li><strong>图内块任务（BIG</strong>）：与ANP类似，BIG任务旨在帮助模型判断两个节点是否存在于同一图中。它涉及随机抽样可能在同一图中或不在同一图中的块对，并预测它们的关系。这有助于模型理解<strong>块与整个图之间的关系</strong>。</li><li><strong>图分类任务（GC）</strong>：使模型能够基于不同平台、架构或优化选项来分类块，特别是在不同编译条件下。GC任务要求模型<strong>区分由于这些不同条件而产生的图和块信息的差异</strong>。</li></ul><h4 id="结构感知模块-Structural-aware-Modeling">结构感知模块 (Structural-aware Modeling)</h4><p>使用消息传递神经网络（MPNN）结合GRU（门控循环单元）更新函数，以提取CFG的全图语义和结构嵌入(the whole graph semantic &amp; structural embedding)<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">g_{ss}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p><img src="/2024/05/04/Order%20Matters/image-20240121215922638.png" alt="image-20240121215922638" style="zoom: 67%;"><ul><li><p><strong>消息传递（Message Passing）</strong></p><ul><li>公式(2)表示的是消息传递阶段。对于图中的每个节点v，它计算了节点v在时间t+1的消息<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>m</mi><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">m^{t+1}_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>。这个消息是通过聚合节点v的所有邻居节点w的信息（使用消息函数M）来得到的。这里，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">h^t_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">h^t_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>分别代表节点v和它的邻居节点w在时间t的嵌入，而<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mrow><mi>v</mi><mi>w</mi></mrow></msub></mrow><annotation encoding="application/x-tex">e_{vw}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是节点v和w之间边的特征。</li><li>公式(5)中，论文使用了多层感知机（Multi-Layer Perceptron, <strong>MLP</strong>）对邻居节点w的嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">h^t_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>进行处理。</li></ul></li><li><p><strong>更新（Update）</strong></p><ul><li><p>公式(3)表示的是更新阶段。在这一步中，节点v的新嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">h^{t+1}_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>是通过更新函数U，结合节点v在时间t的嵌入和它在时间t+1收到的消息来计算的。</p></li><li><p>公式(6)说明了论文中的更新函数是通过GRU实现的，GRU考虑了节点的历史信息<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">h^t_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>和新的消息<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>m</mi><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">m^{t+1}_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>，来学习图的时序信息。</p></li></ul></li><li><p><strong>读出（Readout）</strong></p><ul><li>公式(4)定义了读出函数R，它计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">g_{ss}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。这是通过对图中所有节点v的最终嵌入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">h^T_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>进行聚合来实现的。</li><li>公式(7)中，读出函数是通过对所有节点的初始嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mn>0</mn></msubsup></mrow><annotation encoding="application/x-tex">h^0_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>和最终嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">h^T_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>使用多层感知机（MLP）并进行求和来实现的。这里<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mn>0</mn></msubsup></mrow><annotation encoding="application/x-tex">h^0_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>是由BERT预训练得到的初始块嵌入。</li></ul></li></ul><img src="/2024/05/04/Order%20Matters/image-20240121220012016.png" alt="image-20240121220012016" style="zoom:67%;"><h4 id="顺序感知模块-Order-aware-Modeling"><strong>顺序感知模块</strong> (Order-aware Modeling)</h4><p>通过卷积神经网络（CNN）处理邻接矩阵，以提取CFG节点的顺序信息。</p><p>![image-20240122001434356](Order Matters/image-20240122001434356.png)</p><p>如图，CNN能捕获从(a)到(b)的变化信息，当 CNN 看到大量训练数据时，它具有<strong>平移不变性</strong>（translation invariance）</p><p>对于(b)-&gt;©，与图像放缩类似，在看到足够多的训练数据后，CNN 也可以学习这种伸缩不变性（scale invariance）。</p><p>由于二进制代码函数在不同平台上编译时节点顺序通常不会大改变，<strong>CNN</strong>能够处理由此引起的添加、删除或交换节点等小变化，优势如下：</p><ol><li>使用CNN直接在邻接矩阵上的操作相比于传统的图特征提取算法要快得多。</li><li>CNN可以处理不同大小的输入，这允许模型处理不同大小的图而无需预处理，如填充或裁剪。</li></ol><p>使用具有 3 个残差块的 11 层 Resnet，所有的feature map大小均为3*3，最后使用最大池化层来计算图的顺序嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>o</mi></msub><mo>=</mo><mi>M</mi><mi>a</mi><mi>x</mi><mi>p</mi><mi>o</mi><mi>o</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">(</mo><mi>R</mi><mi>e</mi><mi>s</mi><mi>n</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g_o = Maxpooling(Resnet(A))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mord mathnormal">oo</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">es</span><span class="mord mathnormal">n</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">))</span></span></span></span></p><h3 id="效果">效果</h3><p><strong>数据集：</strong></p><ul><li><p>任务1是跨平台二进制代码检测，目的是确认相同的源代码在不同平台上编译成的CFG是否具有较高的相似性得分。</p><ul><li>与Gemini模型类似，使用孪生网络（siamese network）来减少损失，并使用余弦距离来计算图的相似性。</li></ul></li><li><p>任务2是图分类，对图嵌入进行优化选项分类</p><ul><li>使用softmax函数并选择交叉熵作为损失函数</li></ul></li></ul><p>由于模型具有三个组成部分：语义感知、结构感知和顺序感知，因此进行了不同的实验来找出每个部分的效果。</p><p><img src="/2024/05/04/Order%20Matters/image-20240122012435397.png" alt="image-20240122012435397" style="zoom: 67%;"><img src="/2024/05/04/Order%20Matters/image-20240122012444904.png" alt="image-20240122012444904" style="zoom: 67%;"></p><blockquote><p>表中：</p><ul><li><p>第一个分块是整体模型，包括graph kernel，Gemini以及MPNN模型。</p></li><li><p>第二个分块是语义感知模块的对比实验，分别使用了word2vec[5]，skip thought[6]，以及BERT，其中BERT2是指原始BERT论文中的两个task（即MLM和ANP），BERT4是指在此基础上加入两个graph-level task（BIG和GC）。</p></li><li><p>第三个分块是对顺序感知模块的对比实验，基础CNN模型使用3层CNN以及7、11层的Resnet，CNN_random是对训练集中控制流图的节点顺序随机打乱再进行训练，MPNN_ws是去除控制流图节点中的语义信息（所有block向量设为相同的值）再用MPNN训练。</p></li><li><p>最后是本文的最终模型，即BERT (4 tasks) + MPNN + 11layer Resnet。</p></li></ul></blockquote><blockquote><p>MPNN （即加上结构感知模块）在所有数据集上都优于 Gemini，这是因为 GRU 更新函数可以存储更多信息，因此在所有其他模型中都使用 MPNN。</p></blockquote><p>基于NLP的块预训练特征比手动特征好得多，并且顺序感知模块在两个任务上也有很好的结果。</p><p>在跨平台二进制代码检测任务中，语义信息比顺序信息更有用。不同的CFG可能具有相似的节点顺序，因此仅使用节点顺序信息是不够的。</p><p>最后，最终模型优于所有其他模型。</p><p><strong>分开观察各个模块的有效性</strong>：</p><blockquote><p><img src="/2024/05/04/Order%20Matters/image-20240122014527423.png" alt="image-20240122014527423" style="zoom: 50%;"> <img src="/2024/05/04/Order%20Matters/image-20240122014536191.png" alt="image-20240122014536191" style="zoom:50%;"></p><p>“语义感知（Semantic-aware）”：</p><ul><li>表中的第二块显示，BERT模型的性能优于word2vec和skip thought模型。这是因为BERT在预训练过程中不仅考虑了块级别的预测，还包括了token级别的预测，并且双向Transformer结构能够提取更多有用的信息。</li><li>当BERT模型加入了BIG和GC两个图级任务后，性能有了1%到2%的提升，表明引入图级任务对预训练是有益的。</li><li>图6展示了4个控制流图（CFG）的块嵌入可视化，使用K-means算法将预训练后的块嵌入分成四个类别，每个类别用不同颜色表示。从图中可以观察到，同一控制流图中的块倾向于拥有相同的颜色，而不同控制流图的主要颜色也不同。</li></ul><p>“顺序感知（Order-aware）”：</p><ul><li>表中的第三块显示，基于CNN的模型在两个任务上都取得了良好的效果，其中11层的Resnet略优于3层的CNN和7层的Resnet。</li><li>与不含语义信息的MPNN（MPNN_ws）相比，基于CNN的模型表现出更好的性能。</li><li>节点顺序被随机打乱后，CNN的效果显著下降，这证明CNN模型确实能够学习到图的节点顺序信息。</li><li>图7展示了两个由相同源代码编译而成的CFG变化的例子，尽管左图的节点3在右图中被分成了节点3和4，但其他节点的顺序和边的连接方式保持不变。通过CNN模型的计算，这两个CFG的余弦相似度为0.971，并且在整个平台中的代码检测排名中位列第一。这意味着CNN模型能够从邻接矩阵中有效提取控制流图的节点顺序信息，与假设相符。</li></ul></blockquote><h3 id="结论">结论</h3><p>这篇论文提出了一个新颖的二进制代码图学习框架，包含了语义感知组件、结构感知组件和顺序感知组件。作者观察到，语义信息和节点顺序信息对于表示控制流图（CFGs）都非常重要。为了捕捉语义特征，作者提出了针对CFGs块的BERT预训练，包括两个原始任务MLM和ANP，以及两个额外的图级任务BIG和GC。然后作者使用MPNN来提取结构信息。作者进一步提出了一个基于CNN的模型来捕捉节点顺序信息。作者在两个任务上使用了四个数据集进行了实验，实验结果表明本文提出的模型超越了当时最先进的方法。</p><h3 id="附：部分基础概念解释">附：部分基础概念解释</h3><blockquote><p>由于是第一次精读深度学习相关的技术论文，我翻看了很多基础概念</p></blockquote><h4 id="MLM和NSP">MLM和NSP</h4><p>MLM（Masked language model）和NSP（next sentence prediction）是BERT模型中两个重要的训练任务，它们共同帮助BERT学习理解语言的深层次结构和关系。以下是对这两个任务的具体介绍：</p><p><strong>掩码语言模型任务（MLM）</strong></p><ul><li><strong>目的</strong>：MLM旨在使模型能够更好地理解语言本身的规律和结构。它通过在文本中随机掩盖一些单词（即使用特殊的“[MASK]”标记替换），然后要求模型预测这些掩盖单词的原始值来实现。</li><li><strong>训练过程</strong>：在训练时，BERT模型会尝试根据上下文中的其他单词来猜测被掩盖的单词是什么。例如，在句子“The cat sat on the [MASK]”中，模型需要预测被掩盖的词是“mat”。</li><li><strong>作用</strong>：这种训练方式使得BERT能够有效地学习单词的上下文关系和语义信息，从而更好地理解语言。</li></ul><p><strong>下一个句子预测任务（NSP）</strong></p><ul><li><strong>目的</strong>：NSP的目标是使模型能够理解句子之间的关系。这对于很多NLP任务（如问答系统、自然语言推理等）至关重要。</li><li><strong>训练过程</strong>：在训练时，模型被给予一对句子，并需要判断第二个句子是否在原文中紧跟在第一个句子之后。训练集由两种类型的句子对组成：一种是真实的相邻句子对，另一种是随机组合的非相邻句子对。</li><li><strong>作用</strong>：通过这种方式，BERT学习理解句子之间的逻辑和关系，增强对文本的整体理解能力。</li></ul><h4 id="消息传递神经网络（MPNN）">消息传递神经网络（MPNN）</h4><p>消息传递神经网络（Message Passing Neural Network）是一类图神经网络，它通过在图的节点之间交换信息来学习节点的表示。它们基于以下步骤工作：</p><ol><li><strong>消息传递</strong>：每个节点接收其邻居节点的信息，并根据这些信息生成“消息”。</li><li><strong>聚合</strong>：将所有接收到的消息聚合成单个表示，这可以通过不同的函数实现，如求和、求平均或更复杂的操作。</li><li><strong>更新</strong>：使用聚合的信息来更新节点的状态。</li></ol><p>MPNN的核心思想是通过迭代这些步骤来精炼每个节点的表示，从而捕捉图的结构特征和节点之间的关系。</p><h4 id="门控循环单元（GRU）">门控循环单元（GRU）</h4><p>GRU（gated recurrent unit）是循环神经网络（RNN）的一种变体，用于处理序列数据。与传统的RNN相比，GRU通过引入门控机制来解决梯度消失和梯度爆炸的问题，使得网络能够捕捉长距离依赖关系。GRU包含两个门：</p><ol><li><strong>更新门</strong>：决定状态信息应该如何更新。</li><li><strong>重置门</strong>：决定过去的状态信息在计算新状态时应保留多少。</li></ol><p>在每个时间步，GRU可以选择保留旧状态的信息并融入新输入的信息，这使得它在处理具有复杂依赖结构的数据时非常有效。</p><h4 id="多层感知机-MLP"><strong>多层感知机 (MLP)</strong></h4><ul><li><strong>定义</strong>：多层感知机是一种基础的人工神经网络，由一个输入层、若干隐藏层和一个输出层组成。每一层由多个神经元组成，相邻层之间的神经元通过权重连接。</li><li><strong>功能</strong>：MLP主要用于分类和回归问题，能够识别和建模输入数据中的非线性关系。</li><li><strong>工作原理</strong>：在MLP中，数据从输入层进入，每个神经元对输入进行加权求和，再加上一个偏置项，最后通过激活函数进行非线性转换。这个过程在每个隐藏层中重复进行，直到输出层。在输出层，数据被转换为最终的输出格式（如分类标签或回归值）。</li></ul><h4 id="卷积神经网络-CNN"><strong>卷积神经网络 (CNN)</strong></h4><ul><li><strong>定义</strong>：卷积神经网络是一种深度学习网络，特别适用于处理具有网格结构的数据，如图像（2D网格）和声音（1D网格）。</li><li><strong>功能</strong>：CNN广泛应用于图像和视频识别、图像分类、医学图像分析、自然语言处理等领域。</li><li><strong>工作原理</strong>：CNN通过一系列卷积层、池化层和全连接层处理数据。卷积层使用卷积核提取空间特征，池化层（如最大池化）则减小特征维度并提供一定程度的位置不变性。最后，全连接层将提取的特征用于分类或回归任务。</li></ul><h4 id="最大池化-MaxPooling"><strong>最大池化 (MaxPooling)</strong></h4><ul><li><strong>定义</strong>：最大池化是一种池化操作，常在卷积神经网络中使用，用于减小特征图的空间尺寸。</li><li><strong>功能</strong>：最大池化通过降低参数数量和计算量来减少过拟合，同时保持重要特征。</li><li><strong>工作原理</strong>：最大池化通过在输入特征图的不同区域上应用一个固定大小的窗口，并从每个窗口中选择最大值来实现。这样做可以提取最显著的特征，并且对小的位置变化保持不变性。</li></ul><h4 id="残差网络-ResNet"><strong>残差网络 (ResNet)</strong></h4><ul><li><strong>定义</strong>：ResNet是一种深度卷积神经网络，通过引入残差学习框架来易于优化，并能够构建更深的网络。</li><li><strong>功能</strong>：ResNet在图像识别、分类和其他计算机视觉任务中表现优异。</li><li><strong>工作原理</strong>：在ResNet中，残差块的引入允许输入跳过一些层。每个残差块学习输入和输出的残差（差异），而不是直接学习输出。这帮助网络学习恒等映射，解决了深层网络中的梯度消失问题。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Binary Similarity论文</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文笔记</tag>
      
      <tag>Binary Similarity</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：（Security 22） 关于“二进制函数相似性检测”的调研</title>
    <link href="/2024/05/03/How%20Machine%20Learning%20Is%20Solving%20the%20Binary%20Function%20Similarity%20Problem/"/>
    <url>/2024/05/03/How%20Machine%20Learning%20Is%20Solving%20the%20Binary%20Function%20Similarity%20Problem/</url>
    
    <content type="html"><![CDATA[<blockquote><p>注：部分内容参考自GPT生成的内容</p></blockquote><h2 id="Security-22-关于“二进制函数相似性检测”的调研（个人阅读笔记）">[Security 22] 关于“二进制函数相似性检测”的调研（个人阅读笔记）</h2><p>论文：《<em><a href="https://link.zhihu.com/?target=https%3A//www.usenix.org/conference/usenixsecurity22/presentation/marcelli">How Machine Learning Is Solving the Binary Function Similarity Problem</a></em>》（<strong>Usenix Security 2022</strong>）</p><p>仓库：<a href="https://github.com/Cisco-Talos/binary_function_similarity">https://github.com/Cisco-Talos/binary_function_similarity</a></p><h3 id="动机">动机</h3><p>二进制函数相似性问题在系统安全研究领域扮演着重要角色，现有技术演变很快。但还没有研究能解答一些重要的研究问题，如：使用相同的数据集和相同的指标对不同的方法进行评估时，它们的比较结果如何？与简单的模糊哈希算法相比，新型机器学习解决方案的主要贡献是什么？不同特征集的作用是什么？不同的方法对不同的任务是否更有效？不同的方法对不同的任务是否更有效？跨架构比较是否比单一架构更难解决？在设计新技术的未来方向上，是否有任何特定的研究方向看起来更有前景？</p><p>要回答这些问题，有以下挑战：</p><ol><li>现有研究难以复现或复制先前的结果</li><li>研究结果的不透明性：不同的解决方案通常针对不同的目标定制，使用不同的相似性概念和操作粒度。</li><li>研究方向的不确定性：该领域的研究方向和原因不清晰，研究方法多样且分散</li></ol><blockquote><p>另外，论文在第二章从度量函数相似性的方法和特征表示方法两方面探讨了二进制函数相似性问题</p></blockquote><h3 id="实现的方法"><strong>实现的方法</strong></h3><p>这篇论文<strong>挑选方法的标准</strong>：</p><ol><li>可扩展性和实际应用性</li><li>关注有代表性的方法，而不是具体的论文</li><li>覆盖不同社区：安全、程序语言和机器学习，也考虑工业界</li><li>优先考虑最新趋势</li></ol><p>挑选出的方法，根据研究团队和功能相似性，<strong>划分</strong>如下：</p><p>![image-20240121161933831](How Machine Learning Is Solving the Binary Function Similarity Problem\image-20240121161933831.png)</p><p>作者从中有些许<strong>发现</strong>：</p><ol><li><p>有些论文通过比较得到的结论是错误的，比如将跨架构、基于比对函数的方法与单一架构、比对二进制文件的方法进行比较。</p></li><li><p>不同领域的论文通常都很封闭，很少与其他领域的论文进行比较。</p></li><li><p>一个明显的趋势：随着时间的推移，解决方案的复杂性和机器学习的使用不断增加</p></li></ol><p>从以上挑选的方法中，作者<strong>挑选了十个具有可扩展性、代表性和最新性的最先进方法</strong>，准备进行评估。方法包括：</p><ul><li>Bytes fuzzy hashing: <strong>Catalog1</strong></li><li>CFG fuzzy hashing: <strong>FunctionSimSearch</strong></li><li>Attributed CFG and GNN: <strong>Gemini</strong></li><li>Attributed CFG, GNN, and GMN: <strong>Li et al. 2019</strong></li><li>IR, data flow analysis and neural network: <strong>Zeek</strong></li><li>Assembly code embedding: <strong>Asm2Vec</strong></li><li>Assembly code embedding and self-attentive encoder: <strong>SAFE</strong></li><li>Assembly code embedding, CFG and GNN: <strong>Massarelli et al., 2019</strong></li><li><strong>CodeCMR/BinaryAI</strong></li><li><strong>Trex</strong></li></ul><p><strong>以统一的方式实现了评估的各个阶段</strong>，包括：</p><ul><li><p>二进制分析（IDA Pro 7.3）</p></li><li><p>特征提取（a set of Python scripts using the IDA Pro APIs, Capstone , and NetworkX ）</p></li><li><p>机器学习的实现（Tensorflow 1.14, with the only exception of Trex , which was built on top of Fairseq）</p></li></ul><p><strong>创建了两个新数据集</strong>：旨在捕捉现实世界软件的复杂性和可变性，同时涵盖二进制函数相似性的不同挑战：(i) 多种编译器系列和版本，(ii) 多种编译器优化，(iii) 多种体系结构和位宽，以及 (iv) 不同性质的软件（命令行实用程序与图形用户界面应用程序）。</p><p><strong>确定了六种不同的评估任务</strong>：XO、XC、XC+XB、XA、XA+XO、XM。</p><blockquote><p>O：Optimizations，C：Compiler and Compiler Versions，B：Bitness，A：Architecture，M：Mixed</p></blockquote><h3 id="结果与讨论">结果与讨论</h3><ol><li><p>对Catalog1和FunctionSimSearch进行了<strong>Fuzzy-hashing Comparison</strong>：它们在面对多变量变化的任务时，表现有限。</p></li><li><p><strong>Machine-learning Models Comparison</strong>：</p><p>论文直接提供的结论如下：</p><ul><li>一种机器学习模型，来自 Li 等人的 GNN[40]在六个评估任务中优于所有其他变体，实现了与可扩展性较差的 GMN 版本类似的性能。</li><li>其他基于嵌入的模型[45, 49, 60, 76]显示出较低但相似的准确性。</li><li>Zeek[67]采用直接比较方法，其在处理大型函数时的AUC表现更好。</li><li>Asm2Vec[14]模型在多个任务中的表现并不优于其他模型。</li></ul><p>此外还在4.5节进行了多方面的讨论。</p></li><li><p><strong>Vulnerability Discovery Use Case</strong></p><ul><li><p>使用操作码特征的GMN模型表现最佳，但其可扩展性受限。</p></li><li><p>同时，特定配置下的FSS模型也意外地显示了良好的实用性能，但这种性能并不一定适用于所有配置。</p></li><li><p>表6包含了Netgear R7000固件中易受攻击函数的实际排名结果，显示即使MRR10值很高，实际排名可能仍然很低。</p></li></ul></li></ol><p>最后，在<strong>5 Discussion</strong>部分中，作者回答了开头提出的几个重要的研究问题，比如：</p><ul><li><p><strong>机器学习解决方案与模糊散列方法相比的主要贡献</strong>：机器学习模型即使在多个编译变量同时改变时也能达到高准确率，并且能够从大型训练数据集中受益，这些数据集是基于由编译选项定义的可靠基准。</p></li><li><p><strong>不同特征集的作用</strong>：</p><ul><li>使用基本块特征（例如，ACFG）提供更好的结果，但在精心手工设计的特征和更简单的特征（如基本块操作码的词袋）之间差异很小。</li><li>令人惊讶的是，指令嵌入[45]并没有提高GNN模型的性能，但作者认为需要进行广泛测试来评估其他可能的组合。</li></ul></li><li><p><strong>不同方法在不同任务中的表现</strong>：</p><ul><li>大多数机器学习模型在所有评估任务中表现相似，无论是在相同架构还是跨架构中。</li><li>不需要针对特定任务进行训练，因为使用最通用的任务数据（XM）就能达到接近每个任务最佳的性能。但这对于模糊散列方法并不适用。</li></ul></li><li><p><strong>哪些研究方向更有前途</strong>：深度学习模型、GNN与汇编指令编码器的结合、结合中间表示和数据流信息、训练策略和损失函数等补充方面。</p></li></ul><blockquote><p>更多讨论详见论文</p></blockquote><hr><h3 id="结论">结论</h3><p>本文进行了首次对超过五年来解决二进制函数相似性问题的研究工作的测量研究。作者识别了该研究领域中的一些挑战，以及这些挑战如何使得有意义的比较变得困难，甚至几乎不可能。本文工作旨在弥合这一差距，并帮助社区在这一研究领域获得更清晰的认识。作者希望通过发布所有的实现、数据集和原始结果，社区将拥有一个起点，以开始构建新的方法，并将其与一个共同的框架进行比较，以更好地辨别哪些新颖的方面实际上改进了现有技术状态，以及哪些方面只是看似如此。</p><h3 id="附：部分概念解释">附：部分概念解释</h3><h4 id="一些评估标准">一些评估标准</h4><ol><li><strong>ROC曲线（Receiver Operating Characteristic Curve）</strong>：<ul><li>ROC曲线是一个图形工具，用于评估二元分类器的性能。</li><li>它过将**真阳性率（True Positive Rate，TPR）<strong>和</strong>假阳性率（False Positive Rate，FPR）**作为横纵坐标来描绘分类器在不同阈值下的性能。</li><li>ROC曲线下的面积（AUC）用于量化分类器的整体性能。AUC值越接近1，表明分类器的性能越好。</li></ul></li><li><strong>top-n</strong>：<ul><li>top-n通常用于信息检索和推荐系统中，指的是从一系列项目中选择“最好”的n个项目。</li><li>例如，在推荐系统中，如果你想推荐5个最相关的项目，那么这就是一个top-5的任务。</li><li>在评估时，通常会查看这些top-n项目中有多少是真正相关或准确的。</li></ul></li><li><strong>MRR10（Mean Reciprocal Rank at 10）</strong>：<ul><li>MRR是一种评估信息检索系统效果的指标，特别是当查询返回一个项目列表时。</li><li>MRR10指的是在前10个返回项目中找到第一个正确答案的倒数的平均值。</li><li>例如，如果正确的答案在返回列表的第一个位置，其倒数排名是1；如果在第二个位置，其倒数排名是1/2，依此类推。计算所有查询的这个倒数排名的平均值即得到MRR10。</li></ul></li><li><strong>召回率 (Recall@K)</strong>：<ul><li>这个度量标准关注的是模型能够在前K个结果中检索到多少相关项目。</li><li>例如，如果一个模型能够在前10个返回的项目中找到所有相关项目，则Recall@10将是100%。</li></ul></li></ol><h4 id="pipeline">pipeline</h4><ol><li><strong>Pipeline</strong>:<ul><li>在计算机科学中，pipeline通常指的是一系列数据处理步骤或任务，这些步骤按照特定的顺序组织，每个步骤的输出成为下一个步骤的输入。</li><li>在软件工程和数据科学的背景下，pipeline涉及到从原始数据提取、处理、分析到最终产出的整个过程。例如，一个机器学习pipeline可能包括数据清洗、特征提取、模型训练和预测评估等步骤。</li></ul></li><li><strong>Non-trivial Pipelines</strong>:<ul><li>“Non-trivial”这个词用来描述那些不简单、复杂或需求高的任务或过程。</li><li>当文本中提到“non-trivial pipelines”，它指的是那些在设计和实现上具有一定复杂性和挑战性的数据处理流程。这些pipeline可能包含多个步骤，每个步骤都需要特别的注意，可能涉及复杂的算法或大量的数据处理。</li><li>在二进制函数相似性问题的背景下，non-trivial pipelines可能包括诸如确定函数边界、反汇编代码、提取控制流图等复杂步骤。这些步骤在技术上可能很复杂，需要深入理解底层的计算机架构和编程原理。</li></ul></li></ol><h4 id="“配对选择”（Pair-Selection）">“配对选择”（Pair Selection）</h4><ul><li>配对选择是指如何选择正负样本对（即相似和不相似的函数对）进行模型训练和评估。</li><li>这一方面对于适当的评估至关重要，因为它直接影响到训练任务的难度和评估结果的有效性。</li></ul><h4 id="“词袋”（Bag-of-Words）">“词袋”（Bag of Words）</h4><p>词袋模型将文本（如句子或文档）转换为一个词的集合，忽略了文本中词的顺序和语法结构。</p>]]></content>
    
    
    <categories>
      
      <category>Binary Similarity论文</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文笔记</tag>
      
      <tag>Binary Similarity</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Notice】2024年1月记</title>
    <link href="/2024/01/19/Open2024/"/>
    <url>/2024/01/19/Open2024/</url>
    
    <content type="html"><![CDATA[<h1>【Notice】2024年1月记</h1><p>上周刚考完期末考…研一上学期快结束了。</p><p>一直感觉最近一年没有做科研的状态！好像一直在摆烂。虽然了解了一些以后海外读博的信息，但是光了解有什么用呢。</p><p>本科期间的博客在这里都隐藏起来了，不过可以在CSDN找到它们：<a href="https://blog.csdn.net/qq_40025866?type=blog">Yuhan_2001-CSDN博客</a>。</p><p><strong>接下来我尽量更新一些自己看的文献（或许不是精读），或者一点技术相关的东西。</strong></p><p>Just finished finals last week … First semester of grad school is almost over.</p><p>Been feeling like I haven’t been in a state of doing research for the last year! I seems to be constantly slacking off. Although I have learnt some information about studying for a PhD overseas in the future, what’s the use of just knowing about it.</p><p>The blogs during my undergraduate years are hidden here, but you can find them on CSDN: <a href="https://blog.csdn.net/qq_40025866?type=blog">Yuhan_2001-CSDN Blog</a>.</p><p><strong>Next I’ll try to update with some papers I’ve read (maybe not intensively), or a bit of tech-related stuff.</strong></p>]]></content>
    
    
    <categories>
      
      <category>个人经历</category>
      
    </categories>
    
    
    <tags>
      
      <tag>个人经历</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>欢迎来到我的个人博客！</title>
    <link href="/2021/07/01/hello-world/"/>
    <url>/2021/07/01/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to my blog！</p><p>About me：<a href="https://yuhan2001.github.io/about/">关于页</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>

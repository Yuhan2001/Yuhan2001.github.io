

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.jpg">
  <link rel="icon" href="/img/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Wu Yuhan">
  <meta name="keywords" content="">
  
    <meta name="description" content="论文阅读笔记 AAAI 20：《Order Matters》">
<meta property="og:type" content="article">
<meta property="og:title" content="【论文笔记】Order Matters （AAAI 20）">
<meta property="og:url" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/index.html">
<meta property="og:site_name" content="Yuhan&#39;s blog">
<meta property="og:description" content="论文阅读笔记 AAAI 20：《Order Matters》">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/image-20240121203627839-17058476037342.png">
<meta property="og:image" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/image-20240121204851147-17058476023021.png">
<meta property="og:image" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/image-20240121215922638.png">
<meta property="og:image" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/image-20240121220012016.png">
<meta property="og:image" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/image-20240122001434356.png">
<meta property="og:image" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/image-20240122012435397.png">
<meta property="og:image" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/image-20240122012444904.png">
<meta property="og:image" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/image-20240122014527423.png">
<meta property="og:image" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/image-20240122014536191.png">
<meta property="article:published_time" content="2024-05-03T16:00:00.000Z">
<meta property="article:modified_time" content="2024-05-05T06:22:40.338Z">
<meta property="article:author" content="Wu Yuhan">
<meta property="article:tag" content="论文笔记">
<meta property="article:tag" content="Binary Similarity">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://yuhan2001.github.io/2024/05/04/Order%20Matters/image-20240121203627839-17058476037342.png">
  
  
  <title>【论文笔记】Order Matters （AAAI 20） - Yuhan&#39;s blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://lib.baomitu.com/highlight.js/10.7.3/styles/vs2015.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"yuhan2001.github.io","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"6da93a2e5d7f160e0f40b273ddbbddac","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Yuhan&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/3.jpeg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="【论文笔记】Order Matters （AAAI 20）">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2024-05-04 00:00" pubdate>
        2024年5月4日 凌晨
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      6.1k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      51 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">【论文笔记】Order Matters （AAAI 20）</h1>
            
            <div class="markdown-body">
              <blockquote>
<p>注：部分内容参考自GPT生成的内容</p>
</blockquote>
<h2 id="论文笔记：Order-Matters（AAAI-20）">论文笔记：Order Matters（AAAI 20）</h2>
<p>用于二进制代码相似性检测的语义感知神经网络</p>
<p>论文:《<a target="_blank" rel="noopener" href="https://keenlab.tencent.com/en/whitepapers/Ordermatters.pdf">Order Matters: Semantic-Aware Neural Networks for Binary Code Similarity Detection</a>》（<strong>AAAI 2020</strong>)</p>
<blockquote>
<p>笔记参考：<a target="_blank" rel="noopener" href="https://keenlab.tencent.com/zh/2019/12/10/Tencent-Keen-Security-Lab-Order-Matters/">AAAI-20论文解读：基于图神经网络的二进制代码分析 | 腾讯科恩实验室官方博客 (tencent.com)</a></p>
</blockquote>
<h3 id="动机">动机</h3>
<p>传统方法通常使用图匹配算法，但这些方法慢且不准确。尽管基于神经网络的方法取得了进展（如Gemini），但它们每个基本块都是以人工选择特征的低维嵌入来表示的，通常不能充分捕获二进制代码的语义信息。其次，节点的顺序在表示二进制函数时起着重要作用，而以往的方法并没有设计提取节点顺序的方法。</p>
<blockquote>
<p>另外，在Related Work中提到，(Zuo et al 2018) 使用的NLP模型也有缺点。他们通过修改编译器，在每个生成的汇编块中添加一个基本块特殊注释器，该注释器为每个生成的块注释一个唯一ID。这样，可以将来自同一源代码片段编译的两个基本块视为等效。获取相似块对是一个有监督的过程，不同操作系统或硬件架构需要训练不同的模型</p>
</blockquote>
<h3 id="方法">方法</h3>
<p><strong>提出的模型</strong>：</p>
<p>模型的输入是二进制代码函数的控制流图（CFGs），其中每个块是带有中间表示的token序列。在语义感知上，模型使用BERT预训练接受CFG作为输入，并预训练token嵌入和块嵌入。在结构感知上，使用带GRU更新函数的MPNN来计算图的语义和结构嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">g_{ss}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。在顺序感知上，采用CFG的邻接矩阵作为输入，并使用CNN来计算图的顺序嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">g_{o}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。最后将它们连接起来，并使用一个MLP层来计算图嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mrow><mi>f</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>=</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><msub><mi>g</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub><mo separator="true">,</mo><msub><mi>g</mi><mi>o</mi></msub><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g_{final} =MLP([g_{ss}, g_{o}])</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mord mathnormal mtight">ina</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">([</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">])</span></span></span></span></p>
<img src="/2024/05/04/Order%20Matters/image-20240121203627839-17058476037342.png" srcset="/img/loading.gif" lazyload alt="image-20240121203627839" style="zoom:80%;">
<h4 id="语义感知模块-Semantic-aware-Modeling">语义感知模块 (Semantic-aware Modeling)</h4>
<p>使用BERT进行预训练，包括四个任务：掩码语言模型任务（MLM）、邻接节点预测任务（ANP）、块内图任务（BIG）和图分类任务（GC）。这些任务帮助模型提取CFG的token级别、块级别和图级别的语义信息。</p>
<img src="/2024/05/04/Order%20Matters/image-20240121204851147-17058476023021.png" srcset="/img/loading.gif" lazyload alt="image-20240121204851147" style="zoom: 80%;">
<ul>
<li><strong>掩码语言模型任务（MLM）</strong>：通过在输入层掩盖token并在输出层预测它们来提取<strong>块内的语义信息</strong>。这是一个自监督任务，模型在训练过程中某些token会被隐藏，模型必须基于其他token提供的上下文来预测缺失的token。</li>
<li><strong>邻接节点预测任务（ANP）</strong>：因为块的信息不仅与块本身的内容相关，还与其邻近的块相关，ANP任务旨在让模型学习这种<strong>邻接信息</strong>。它涉及提取图中所有相邻块对，并在同一图中随机抽样多个块对，以预测它们是否相邻。</li>
<li><strong>图内块任务（BIG</strong>）：与ANP类似，BIG任务旨在帮助模型判断两个节点是否存在于同一图中。它涉及随机抽样可能在同一图中或不在同一图中的块对，并预测它们的关系。这有助于模型理解<strong>块与整个图之间的关系</strong>。</li>
<li><strong>图分类任务（GC）</strong>：使模型能够基于不同平台、架构或优化选项来分类块，特别是在不同编译条件下。GC任务要求模型<strong>区分由于这些不同条件而产生的图和块信息的差异</strong>。</li>
</ul>
<h4 id="结构感知模块-Structural-aware-Modeling">结构感知模块 (Structural-aware Modeling)</h4>
<p>使用消息传递神经网络（MPNN）结合GRU（门控循环单元）更新函数，以提取CFG的全图语义和结构嵌入(the whole graph semantic &amp; structural embedding)<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">g_{ss}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p>
<img src="/2024/05/04/Order%20Matters/image-20240121215922638.png" srcset="/img/loading.gif" lazyload alt="image-20240121215922638" style="zoom: 67%;">
<ul>
<li>
<p><strong>消息传递（Message Passing）</strong></p>
<ul>
<li>公式(2)表示的是消息传递阶段。对于图中的每个节点v，它计算了节点v在时间t+1的消息<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>m</mi><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">m^{t+1}_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>。这个消息是通过聚合节点v的所有邻居节点w的信息（使用消息函数M）来得到的。这里，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">h^t_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">h^t_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>分别代表节点v和它的邻居节点w在时间t的嵌入，而<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mrow><mi>v</mi><mi>w</mi></mrow></msub></mrow><annotation encoding="application/x-tex">e_{vw}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是节点v和w之间边的特征。</li>
<li>公式(5)中，论文使用了多层感知机（Multi-Layer Perceptron, <strong>MLP</strong>）对邻居节点w的嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">h^t_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>进行处理。</li>
</ul>
</li>
<li>
<p><strong>更新（Update）</strong></p>
<ul>
<li>
<p>公式(3)表示的是更新阶段。在这一步中，节点v的新嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">h^{t+1}_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>是通过更新函数U，结合节点v在时间t的嵌入和它在时间t+1收到的消息来计算的。</p>
</li>
<li>
<p>公式(6)说明了论文中的更新函数是通过GRU实现的，GRU考虑了节点的历史信息<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">h^t_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>和新的消息<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>m</mi><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">m^{t+1}_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>，来学习图的时序信息。</p>
</li>
</ul>
</li>
<li>
<p><strong>读出（Readout）</strong></p>
<ul>
<li>公式(4)定义了读出函数R，它计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">g_{ss}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。这是通过对图中所有节点v的最终嵌入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">h^T_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>进行聚合来实现的。</li>
<li>公式(7)中，读出函数是通过对所有节点的初始嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mn>0</mn></msubsup></mrow><annotation encoding="application/x-tex">h^0_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>和最终嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">h^T_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>使用多层感知机（MLP）并进行求和来实现的。这里<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mn>0</mn></msubsup></mrow><annotation encoding="application/x-tex">h^0_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>是由BERT预训练得到的初始块嵌入。</li>
</ul>
</li>
</ul>
<img src="/2024/05/04/Order%20Matters/image-20240121220012016.png" srcset="/img/loading.gif" lazyload alt="image-20240121220012016" style="zoom:67%;">
<h4 id="顺序感知模块-Order-aware-Modeling"><strong>顺序感知模块</strong> (Order-aware Modeling)</h4>
<p>通过卷积神经网络（CNN）处理邻接矩阵，以提取CFG节点的顺序信息。</p>
<img src="/2024/05/04/Order%20Matters/image-20240122001434356.png" srcset="/img/loading.gif" lazyload alt="image-20240122001434356" style="zoom:80%;">
<p>如图，CNN能捕获从(a)到(b)的变化信息，当 CNN 看到大量训练数据时，它具有<strong>平移不变性</strong>（translation invariance）</p>
<p>对于(b)-&gt;©，与图像放缩类似，在看到足够多的训练数据后，CNN 也可以学习这种伸缩不变性（scale invariance）。</p>
<p>由于二进制代码函数在不同平台上编译时节点顺序通常不会大改变，<strong>CNN</strong>能够处理由此引起的添加、删除或交换节点等小变化，优势如下：</p>
<ol>
<li>使用CNN直接在邻接矩阵上的操作相比于传统的图特征提取算法要快得多。</li>
<li>CNN可以处理不同大小的输入，这允许模型处理不同大小的图而无需预处理，如填充或裁剪。</li>
</ol>
<p>使用具有 3 个残差块的 11 层 Resnet，所有的feature map大小均为3*3，最后使用最大池化层来计算图的顺序嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>o</mi></msub><mo>=</mo><mi>M</mi><mi>a</mi><mi>x</mi><mi>p</mi><mi>o</mi><mi>o</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">(</mo><mi>R</mi><mi>e</mi><mi>s</mi><mi>n</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g_o = Maxpooling(Resnet(A))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mord mathnormal">oo</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">es</span><span class="mord mathnormal">n</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">))</span></span></span></span></p>
<h3 id="效果">效果</h3>
<p><strong>数据集：</strong></p>
<ul>
<li>
<p>任务1是跨平台二进制代码检测，目的是确认相同的源代码在不同平台上编译成的CFG是否具有较高的相似性得分。</p>
<ul>
<li>与Gemini模型类似，使用孪生网络（siamese network）来减少损失，并使用余弦距离来计算图的相似性。</li>
</ul>
</li>
<li>
<p>任务2是图分类，对图嵌入进行优化选项分类</p>
<ul>
<li>使用softmax函数并选择交叉熵作为损失函数</li>
</ul>
</li>
</ul>
<p>由于模型具有三个组成部分：语义感知、结构感知和顺序感知，因此进行了不同的实验来找出每个部分的效果。</p>
<p><img src="/2024/05/04/Order%20Matters/image-20240122012435397.png" srcset="/img/loading.gif" lazyload alt="image-20240122012435397"><img src="/2024/05/04/Order%20Matters/image-20240122012444904.png" srcset="/img/loading.gif" lazyload alt="image-20240122012444904" style="zoom: 67%;"></p>
<blockquote>
<p>表中：</p>
<ul>
<li>
<p>第一个分块是整体模型，包括graph kernel，Gemini以及MPNN模型。</p>
</li>
<li>
<p>第二个分块是语义感知模块的对比实验，分别使用了word2vec[5]，skip thought[6]，以及BERT，其中BERT2是指原始BERT论文中的两个task（即MLM和ANP），BERT4是指在此基础上加入两个graph-level task（BIG和GC）。</p>
</li>
<li>
<p>第三个分块是对顺序感知模块的对比实验，基础CNN模型使用3层CNN以及7、11层的Resnet，CNN_random是对训练集中控制流图的节点顺序随机打乱再进行训练，MPNN_ws是去除控制流图节点中的语义信息（所有block向量设为相同的值）再用MPNN训练。</p>
</li>
<li>
<p>最后是本文的最终模型，即BERT (4 tasks) + MPNN + 11layer Resnet。</p>
</li>
</ul>
</blockquote>
<blockquote>
<p>MPNN （即加上结构感知模块）在所有数据集上都优于 Gemini，这是因为 GRU 更新函数可以存储更多信息，因此在所有其他模型中都使用 MPNN。</p>
</blockquote>
<p>基于NLP的块预训练特征比手动特征好得多，并且顺序感知模块在两个任务上也有很好的结果。</p>
<p>在跨平台二进制代码检测任务中，语义信息比顺序信息更有用。不同的CFG可能具有相似的节点顺序，因此仅使用节点顺序信息是不够的。</p>
<p>最后，最终模型优于所有其他模型。</p>
<p><strong>分开观察各个模块的有效性</strong>：</p>
<blockquote>
<p><img src="/2024/05/04/Order%20Matters/image-20240122014527423.png" srcset="/img/loading.gif" lazyload alt="image-20240122014527423" style="zoom: 50%;"> <img src="/2024/05/04/Order%20Matters/image-20240122014536191.png" srcset="/img/loading.gif" lazyload alt="image-20240122014536191" style="zoom:50%;"></p>
<p>“语义感知（Semantic-aware）”：</p>
<ul>
<li>表中的第二块显示，BERT模型的性能优于word2vec和skip thought模型。这是因为BERT在预训练过程中不仅考虑了块级别的预测，还包括了token级别的预测，并且双向Transformer结构能够提取更多有用的信息。</li>
<li>当BERT模型加入了BIG和GC两个图级任务后，性能有了1%到2%的提升，表明引入图级任务对预训练是有益的。</li>
<li>图6展示了4个控制流图（CFG）的块嵌入可视化，使用K-means算法将预训练后的块嵌入分成四个类别，每个类别用不同颜色表示。从图中可以观察到，同一控制流图中的块倾向于拥有相同的颜色，而不同控制流图的主要颜色也不同。</li>
</ul>
<p>“顺序感知（Order-aware）”：</p>
<ul>
<li>表中的第三块显示，基于CNN的模型在两个任务上都取得了良好的效果，其中11层的Resnet略优于3层的CNN和7层的Resnet。</li>
<li>与不含语义信息的MPNN（MPNN_ws）相比，基于CNN的模型表现出更好的性能。</li>
<li>节点顺序被随机打乱后，CNN的效果显著下降，这证明CNN模型确实能够学习到图的节点顺序信息。</li>
<li>图7展示了两个由相同源代码编译而成的CFG变化的例子，尽管左图的节点3在右图中被分成了节点3和4，但其他节点的顺序和边的连接方式保持不变。通过CNN模型的计算，这两个CFG的余弦相似度为0.971，并且在整个平台中的代码检测排名中位列第一。这意味着CNN模型能够从邻接矩阵中有效提取控制流图的节点顺序信息，与假设相符。</li>
</ul>
</blockquote>
<h3 id="结论">结论</h3>
<p>这篇论文提出了一个新颖的二进制代码图学习框架，包含了语义感知组件、结构感知组件和顺序感知组件。作者观察到，语义信息和节点顺序信息对于表示控制流图（CFGs）都非常重要。为了捕捉语义特征，作者提出了针对CFGs块的BERT预训练，包括两个原始任务MLM和ANP，以及两个额外的图级任务BIG和GC。然后作者使用MPNN来提取结构信息。作者进一步提出了一个基于CNN的模型来捕捉节点顺序信息。作者在两个任务上使用了四个数据集进行了实验，实验结果表明本文提出的模型超越了当时最先进的方法。</p>
<h3 id="附：部分基础概念解释">附：部分基础概念解释</h3>
<blockquote>
<p>由于是第一次精读深度学习相关的技术论文，我翻看了很多基础概念</p>
</blockquote>
<h4 id="MLM和NSP">MLM和NSP</h4>
<p>MLM（Masked language model）和NSP（next sentence prediction）是BERT模型中两个重要的训练任务，它们共同帮助BERT学习理解语言的深层次结构和关系。以下是对这两个任务的具体介绍：</p>
<p><strong>掩码语言模型任务（MLM）</strong></p>
<ul>
<li><strong>目的</strong>：MLM旨在使模型能够更好地理解语言本身的规律和结构。它通过在文本中随机掩盖一些单词（即使用特殊的“[MASK]”标记替换），然后要求模型预测这些掩盖单词的原始值来实现。</li>
<li><strong>训练过程</strong>：在训练时，BERT模型会尝试根据上下文中的其他单词来猜测被掩盖的单词是什么。例如，在句子“The cat sat on the [MASK]”中，模型需要预测被掩盖的词是“mat”。</li>
<li><strong>作用</strong>：这种训练方式使得BERT能够有效地学习单词的上下文关系和语义信息，从而更好地理解语言。</li>
</ul>
<p><strong>下一个句子预测任务（NSP）</strong></p>
<ul>
<li><strong>目的</strong>：NSP的目标是使模型能够理解句子之间的关系。这对于很多NLP任务（如问答系统、自然语言推理等）至关重要。</li>
<li><strong>训练过程</strong>：在训练时，模型被给予一对句子，并需要判断第二个句子是否在原文中紧跟在第一个句子之后。训练集由两种类型的句子对组成：一种是真实的相邻句子对，另一种是随机组合的非相邻句子对。</li>
<li><strong>作用</strong>：通过这种方式，BERT学习理解句子之间的逻辑和关系，增强对文本的整体理解能力。</li>
</ul>
<h4 id="消息传递神经网络（MPNN）">消息传递神经网络（MPNN）</h4>
<p>消息传递神经网络（Message Passing Neural Network）是一类图神经网络，它通过在图的节点之间交换信息来学习节点的表示。它们基于以下步骤工作：</p>
<ol>
<li><strong>消息传递</strong>：每个节点接收其邻居节点的信息，并根据这些信息生成“消息”。</li>
<li><strong>聚合</strong>：将所有接收到的消息聚合成单个表示，这可以通过不同的函数实现，如求和、求平均或更复杂的操作。</li>
<li><strong>更新</strong>：使用聚合的信息来更新节点的状态。</li>
</ol>
<p>MPNN的核心思想是通过迭代这些步骤来精炼每个节点的表示，从而捕捉图的结构特征和节点之间的关系。</p>
<h4 id="门控循环单元（GRU）">门控循环单元（GRU）</h4>
<p>GRU（gated recurrent unit）是循环神经网络（RNN）的一种变体，用于处理序列数据。与传统的RNN相比，GRU通过引入门控机制来解决梯度消失和梯度爆炸的问题，使得网络能够捕捉长距离依赖关系。GRU包含两个门：</p>
<ol>
<li><strong>更新门</strong>：决定状态信息应该如何更新。</li>
<li><strong>重置门</strong>：决定过去的状态信息在计算新状态时应保留多少。</li>
</ol>
<p>在每个时间步，GRU可以选择保留旧状态的信息并融入新输入的信息，这使得它在处理具有复杂依赖结构的数据时非常有效。</p>
<h4 id="多层感知机-MLP"><strong>多层感知机 (MLP)</strong></h4>
<ul>
<li><strong>定义</strong>：多层感知机是一种基础的人工神经网络，由一个输入层、若干隐藏层和一个输出层组成。每一层由多个神经元组成，相邻层之间的神经元通过权重连接。</li>
<li><strong>功能</strong>：MLP主要用于分类和回归问题，能够识别和建模输入数据中的非线性关系。</li>
<li><strong>工作原理</strong>：在MLP中，数据从输入层进入，每个神经元对输入进行加权求和，再加上一个偏置项，最后通过激活函数进行非线性转换。这个过程在每个隐藏层中重复进行，直到输出层。在输出层，数据被转换为最终的输出格式（如分类标签或回归值）。</li>
</ul>
<h4 id="卷积神经网络-CNN"><strong>卷积神经网络 (CNN)</strong></h4>
<ul>
<li><strong>定义</strong>：卷积神经网络是一种深度学习网络，特别适用于处理具有网格结构的数据，如图像（2D网格）和声音（1D网格）。</li>
<li><strong>功能</strong>：CNN广泛应用于图像和视频识别、图像分类、医学图像分析、自然语言处理等领域。</li>
<li><strong>工作原理</strong>：CNN通过一系列卷积层、池化层和全连接层处理数据。卷积层使用卷积核提取空间特征，池化层（如最大池化）则减小特征维度并提供一定程度的位置不变性。最后，全连接层将提取的特征用于分类或回归任务。</li>
</ul>
<h4 id="最大池化-MaxPooling"><strong>最大池化 (MaxPooling)</strong></h4>
<ul>
<li><strong>定义</strong>：最大池化是一种池化操作，常在卷积神经网络中使用，用于减小特征图的空间尺寸。</li>
<li><strong>功能</strong>：最大池化通过降低参数数量和计算量来减少过拟合，同时保持重要特征。</li>
<li><strong>工作原理</strong>：最大池化通过在输入特征图的不同区域上应用一个固定大小的窗口，并从每个窗口中选择最大值来实现。这样做可以提取最显著的特征，并且对小的位置变化保持不变性。</li>
</ul>
<h4 id="残差网络-ResNet"><strong>残差网络 (ResNet)</strong></h4>
<ul>
<li><strong>定义</strong>：ResNet是一种深度卷积神经网络，通过引入残差学习框架来易于优化，并能够构建更深的网络。</li>
<li><strong>功能</strong>：ResNet在图像识别、分类和其他计算机视觉任务中表现优异。</li>
<li><strong>工作原理</strong>：在ResNet中，残差块的引入允许输入跳过一些层。每个残差块学习输入和输出的残差（差异），而不是直接学习输出。这帮助网络学习恒等映射，解决了深层网络中的梯度消失问题。</li>
</ul>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Binary-Similarity%E8%AE%BA%E6%96%87/">Binary Similarity论文</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
                    
                      <a class="hover-with-bg" href="/tags/Binary-Similarity/">Binary Similarity</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均为博客作者本人编写整理，转载请联系作者！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/05/11/jTrans/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【论文笔记】jTrans（ISSTA 22）</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/05/03/How%20Machine%20Learning%20Is%20Solving%20the%20Binary%20Function%20Similarity%20Problem/">
                        <span class="hidden-mobile">【论文笔记】关于“二进制函数相似性检测”的调研（Security 22）</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.16/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"gI9ZupcpBXTdxJvxZZrrxydJ-gzGzoHsz","appKey":"Eew9XWCbMWVcoNrQzrg87EP3","path":"window.location.pathname","placeholder":"说点什么","avatar":"mp","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":"trut","recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          Fluid.plugins.initFancyBox('#valine .vcontent img:not(.vemoji)');
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="" target="_blank" rel="nofollow noopener"><span>Copyrights © 2024 Yuhan</span></a> <i class="iconfont icon-love"></i> <a href="" target="_blank" rel="nofollow noopener"><span>Xiayan</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js" ></script>
  
  
    <script  src="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js" ></script>
  
  
    <script defer src="https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- KaTeX -->
    <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.15.3/katex.min.css" />
  








  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?6da93a2e5d7f160e0f40b273ddbbddac";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
